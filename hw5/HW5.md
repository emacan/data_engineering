## Column Type Annotation using ChatGPT
L'articolo esplora l'uso di ChatGPT per la Column Type Annotation (CTA), un compito importante nella pre-elaborazione dei dati. La CTA assegna etichette semantiche (ad esempio, "Nome ristorante", "Codice postale") alle colonne di una tabella in base ai valori contenuti. I metodi tradizionali si basano su knowledge graph o modelli di linguaggio pre-addestrati (PLM) come BERT, che richiedono spesso grandi quantità di dati di addestramento specifici per il compito. I modelli linguistici di grandi dimensioni (LLM) come ChatGPT offrono potenzialmente prestazioni migliori con meno o addirittura nessun dato di addestramento specifico per il compito, grazie al loro pre-addestramento su enormi quantità di testo. L'efficacia dell'utilizzo di modelli linguistici di grandi dimensioni (LLM) come ChatGPT dipende fortemente dalla formulazione dei prompt. Anche piccole variazioni nelle parole possono influenzare le prestazioni del modello. L'approccio che viene poi seguito dasgli autori dell'articolo è quello di chiedere al modello prima di analizzare l'input e poi di identificare la classe che meglio rappresenta il significato dell'input. L'articolo propone tre tipi di prompt: di tipo Column, descrive il compito come "Classifica la colonna...", seguito dall'elenco dei possibili tipi semantici e dai valori della colonna; di tipo Text, presenta il compito come una generica classificazione di testo, chiedendo di "Classificare il testo..." seguito dai valori della colonna; di tipo Table, fornisce l'intera tabella (prime 5 righe) a ChatGPT, chiedendo di classificare tutte le colonne contemporaneamente. I formati "Text" e "Column" ottengono prestazioni simili (circa 45-47% di F1-score), con "Text" leggermente migliore. Il formato "Table" ottiene risultati inferiori (circa 8% in meno), suggerendo che ChatGPT ha difficoltà a gestire input più lunghi e complessi. Invece di utilizzare semplici prompts gli autori dell'articolo hanno tentato di ottenere risultati migliori andando ad aggiugnere un set di istruzioni esplicite in coda al prompt sulla base del tipo di prompt che viene utilizzato. L'aggiunta di istruzioni migliora significativamente le prestazioni per tutti e tre i formati. Il miglioramento è particolarmente evidente per il formato "table", con un aumento di 34.31 punti F1, raggiungendo un punteggio di 80.16. Questo suggerisce che fornire istruzioni chiare aiuta ChatGPT a comprendere meglio il compito e a sfruttare le informazioni contestuali della tabella. Un altro approccio è quello di utilizzare i Message Roles: i modelli di chat come GPT-3.5-turbo e GPT-4 difatti supportano i ruoli dei messaggi (system, user, AI) per strutturare le conversazioni. I messaggi di sistema vengono utilizzati per fornire descrizioni dei task e istruzioni al modello, i messaggi utente vengono utilizzati per presentare il compito di annotazione effettivo, mentre i messaggi AI contengono le risposte del modello. I risultati sperimentali mostrano un aumento del 28% al 39% nel punteggio micro-F1 rispetto al modello di base, indicando che l'uso dei ruoli dei messaggi è benefico. L'approccio successivo si basa sull'osservazione che le prestazioni degli LLM possono essere migliorate fornendo dimostrazioni del compito che devono svolgere (few-shot learning). Ovviamente gli esempi vengono scelti casualmente dal training set per evitare di fornire informazioni sulle etichette reali. Un messaggio utente viene utilizzato per presentare il compito di dimostrazione e un messaggio AI per mostrare la risposta attesa (etichette reali). Successivamente, viene presentato l'esempio di test effettivo utilizzando un messaggio utente. L'aggiunta di dimostrazioni (1 o 5 esempi) migliora le prestazioni in tutti e tre i formati di prompt (column, text, table) rispetto allo scenario zero-shot. Il miglioramento varia dal 29% al 42% in termini di F1-score. In generale, più esempi vengono forniti, migliori sono le prestazioni, tranne nel caso del formato "table", dove l'aumento da 1 a 5 esempi è marginale (0.39%). L'ultimo approccio cerca invece di rispondere al seguente problema: in scenari reali, il numero di possibili tipi semantici per le colonne può essere molto elevato (ad esempio, 91 nel benchmark SOTAB o 255 in WikiTables). Includere tutte queste etichette nei prompt può renderli troppo lunghi e complessi, influenzando negativamente le prestazioni di ChatGPT. Le tabelle appartenenti a domini tematici diversi tendono però ad avere tipi di colonne caratteristici (ad esempio, un hotel avrà un numero di telefono e un indirizzo, ma non una data di uscita di un album musicale). La pipeline sfrutta questa informazione per semplificare il compito di annotazione. In particolare nel primo step viene chiesto a ChatGPT di predire il dominio tematico della tabella in input, mentre nel secondo step viene selezionato il sottoinsieme di etichette semantiche rilevanti per il dominio previsto e viene chiesto a ChatGPT di annotare le colonne della tabella utilizzando solo queste etichette. Questa pipeline migliora di molto le prestazioni di ChatGPT nella CTA, specialmente con vocabolari estesi. Il primo step raggiunge in tutti i casi, sia 0 shot che few shots una F1 del 95%. Nel secondo step si raggiunge una F1 di circa il 90%, i miglioramenti sono limitati con few-shot learning. Infine, vengono confrontati i risultati di ChatGPT con Random Forest, RoBERTa fine-tuned e DODUO (state-of-the-art per CTA).
ChatGPT zero-shot supera Random Forest e RoBERTa con pochi esempi di training. RoBERTa con più esempi di training si avvicina a ChatGPT, ma rimane inferiore. DODUO, pur essendo lo stato dell'arte, performa peggio in questo scenario specifico.

## Can foundation models wrangle your data?
L'articolo si pone la domanda se i Foundation Models (FMs) possano essere applicati con successo anche a compiti di gestione dei dati strutturati, come la pulizia e l'integrazione dei dati. L'idea presentata dall'articolo si concentra pertanto nel "tradurre" i compiti di gestione dei dati in attività di generazione di testo, e ciò viene ottenuto tramite il seguente processo: Prima di tutto si procede con una serializzazione dei dati tabulari in testo, ovvero convertire le entry di un dataset in rappresentazioni testuali In questo caso per esempio valori nulli possono essere serializzati come stringhe vuote. In seguito abbiamo la riformulazione dei task come task di generazione di testo, ovvero creare prompt per i task di entity matching, rilevamento di errori e imputazione dei dati. Infine si conclude con la costruzione di esempi dimostrativi (few-shot prompting), che consiste nel fornire esempi al modello per aiutarlo ad apprendere nuovi task. Nel contesto del record linkage ci interessa vedere che risultati ha il modello proposto dagli autori nel caso per l'appunto del task dell'entity matching. Anche in questo caso la metrica utilizzata è l'F1 score. In primo luogo sono confrontate le performance di diversi modelli (Magellan, Ditto, GPT3-175B) nel compito di Entity Matching. Per GPT3-175B, vengono mostrati i risultati sia senza dimostrazioni (k=0) che con 10 dimostrazioni (k=10). In generale, Magellan e Ditto mostrano prestazioni molto buone, spesso raggiungendo o avvicinandosi all'F1-score perfetto di 100, mentre le prestazioni di GPT3-175B migliorano significativamente con l'aggiunta di dimostrazioni (k=10) e, in alcuni casi, raggiunge prestazioni paragonabili a Magellan e Ditto. In secondo luogo si analizza l'impatto di diverse scelte nel prompt tuning (selezione degli attributi, formattazione del prompt e selezione degli esempi) sulle prestazioni di entity matching. La selezione degli attributi migliora le prestazioni in modo significativo rispetto a quando non vengono selezionati (valori di F1 di 98 e 100 per due dataset). Questo indica che rimuovere attributi rumorosi o irrilevanti aiuta il modello a concentrarsi sulle informazioni chiave per l'entity matching. Rimuovere invece i nomi degli attributi dal prompt ha un impatto minore, suggerendo che il modello può inferire il significato degli attributi dal loro contesto. Per quanto riguarda la scelta del prompt si individua una differenza di 9.41 punti F1 solo con una leggera variazione nella formulazione del prompt ("the same?" vs. "equivalent?"). Infine, per quanto riguarda la selezione manuale di esempi di dimostrazione si individuano risultati migliori rispetto alla selezione casuale, con un miglioramento medio di 14.7 punti F1. Questo sottolinea l'importanza di fornire al modello esempi informativi e pertinenti.