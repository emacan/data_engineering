# Parte 1
La tesi fondamentale di Andrew Ng è quella che, nell'ambito dell'intelligenza artificiale e in particolare l’allenamento di neti neurali, riguarda la qualità dei dati come aspetto prioritario rispetto alla mera quantità. Questa riflessione è secondo me particolarmente pertinente in situazioni in cui la complessità del dominio non richiede necessariamente un'enorme mole di dati, ma piuttosto richiede dati accurati e privi di distorsioni, dati che possano superare in efficacia volumi enormi ma in realtà poco informativi, ma è pertinente anche nel caso in cui ci confrontiamo con situazioni in cui la dimensione del dataset rischia di diventare eccessivamente grande, come nel caso dei modelli di computer vision, soprattutto considerando la crescente richiesta di potenza computazionale. Inoltre, concordo pienamente sulla visione del data-centric AI come strumento efficace per affrontare i comportamenti di bias nelle reti neurali, soprattutto nei casi dei grandi modelli foundational. Questo problema è per me evidente in modelli per la creazione di immagini come DALL-E, dove emergono distorsioni culturali e di genere, come il sovrappeso di dettagli su minoranze o stereotipi culturali che possono influenzare l'output creativo. L'idea di porre maggiore enfasi sulla cura e l'ingegnerizzazione del dataset rappresenta un passo significativo verso la creazione di sistemi più equi e accurati, ma anche più etici. Per concludere, rimango scettico sulla proposta di spostare la responsabilità ai clienti per la costruzione di modelli personalizzati. Sebbene l'idea di democratizzare e standardissare la creazione di modelli AI sia allettante, la sfida pratica risiede nell'empowerment degli utenti, nello sforzo nel fornire loro non solo gli strumenti tecnici ma anche la comprensione dell’intero complesso, di per se gia molto complesso.

# Parte 2
Ho scelto l'articolo "Can Foundation Models wrangle your data?" perchè nell'ultimo homework del corso è concesso utilizzare nel contesto della data integration una soluzione custom basta su ChatGPT e mi volevo documentare al riguardo. Difatti, questo articolo si pone la domanda se i Foundation Models (FMs) possano essere applicati con successo anche a compiti di gestione dei dati strutturati, come la pulizia e l'integrazione dei dati. Uno dei primi problemi che viene in mente è però per esempio che i simboli presenti nei dati strutturati (date, numeri, codici) sono meno frequenti nel linguaggio naturale, quindi non è ovvio che gli FM possano gestirli efficacemente. Un altro problema che viene subito in mente è che gli FM prendono in input testo, non tabelle, e di conseguenza anche il problema di come poter rappresentare efficaciemente il task di Entity Matching in linguaggio naturale, ad esempio potrebbe bastare porre la domanda "I prodotti A e B sono uguali?" ?. L'idea presentata dall'articolo si concentra pertanto nel "tradurre" i compiti di gestione dei dati in attività di generazione di testo, e ciò viene ottenuto tramite il seguente processo: Prima di tutto si procede con una `serializzazione` dei dati tabulari in testo, ovvero convertire le entry di un dataset in rappresentazioni testuali, il che è indispensabile per il problema citato prima. In questo caso per esempio valori nulli possono essere serializzati come stringhe vuote. Nota importante: la serializzazione è applicata solo a un sottoinsieme di attributi rilevanti per il task. In seguito abbiamo la `riformulazione dei task` come task di generazione di testo, ovvero creare prompt per i task di entity matching, rilevamento di errori e imputazione dei dati. Infine si conclude con la `costruzione di esempi dimostrativi` (per few-shot prompting), che consiste nel fornire esempi al modello per aiutarlo ad apprendere nuovi task. L'idea di sfruttare modelli come ChatGPT per compiti di integrazione dei dati apre secondo me un mondo di possibilità. Bisogna però essere consapevoli anche delle sfide oltre quelle di cui sopra. La serializzazione dei dati tabulari in testo, ad esempio, mi sembra un passaggio molto delicato, come facciamo a garantire che le informazioni importanti non sono perse durante la conversione? Come possiamo rendere questi modelli veramente efficaci nel gestire dati strutturati, che sono invece così tanto diversi dal linguaggio naturale su cui sono stati addestrati? Credo che i Foundation Models abbiano un enorme potenziale per rivoluzionare il modo in cui lavoriamo con i dati, ma forse attualmente ci sono metodi migliori, anche per allontanarci dall'idea che i LLM siano la risposta a tutto.